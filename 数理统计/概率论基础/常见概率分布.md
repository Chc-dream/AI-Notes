# Probability Distribution | 常见概率分布

# Uniform DIstribution | 均匀分布

均匀分布式是定义在区间 $[a,b] (a<b)$ 上连续变量的简单概率分布。对于离散随机变量 $X$，假设其有 $k$ 个取值：$x_{1}, x_{2}, \cdots, x_{k}$，则均匀分布的概率密度函数 PMF 为：

$$
p\left(X=x_{i}\right)=\frac{1}{k}, \quad i=1,2, \cdots, k
$$

对于连续随机变量 $X$ 而言，其概率密度函数 PDF 为：

$$
p(X=x)=\left\{\begin{array}{ll}{0,} & {x \notin[a, b]} \\ {\frac{1}{b-a},} & {x \in[a, b]}\end{array}\right.
$$

推导其期望与方差为：

$$
E(X)=\frac{1}{2}(a+b) \\
D(X)=\frac{(b-a)^2}{12}
$$

不难发现，如果变量 $x$ 服从均匀分布 $U(x|0,1)$ 且 $a<b$，则 $a+(b-a)x$ 服从均匀分布 $U(x|a,b)$.

# Bernoulli Distribution | 伯努利分布

伯努利分布是关于布尔变量 $x \in \{0,1\}$ 的概率分布，其连续参数 $\phi \in [0,1]$ 表示变量 $x=1$ 的概率。

$$
P(x|\phi)=Bern(x|\phi)=\phi^x(1-x)^{(1-x)} \\
E[x] = \phi \\
Var[x] = \phi(1-\phi)
$$

# Binomial Distribution | 二项分布

二项分布用以描述 $n$ 次独立的伯努利实验中有 $x$ 次成功的概率，其中每次伯努利实验成功的概率为 $\phi \in [0,1]$:

$$
p(X=x)=\frac{n !}{x !(n-x) !} \phi^{x}(1-\phi)^{n-x}, x \in\{0,1, \cdots, n\}
$$

当 $n=1$ 时，二项分布退化为伯努利分布。二项分布的典型例子是扔硬币，硬币正面朝上概率为 $p$, 重复扔 $n$ 次硬币，$k$ 次为正面的概率即为一个二项分布概率。

二项分布的期望 $\mathbb{E}[X]=n \phi$，方差则是 $\operatorname{Var}[X]=n \phi(1-\phi)$。

# Categorical/Multinomial Distribution | 多项分布

比如扔骰子，不同于扔硬币，骰子有 6 个面对应 6 个不同的点数，这样单次每个点数朝上的概率都是 $1/6$，即对应 p1~p6，它们的值不一定都是 1/6，只要和为 1 且互斥即可，比如一个形状不规则的骰子；重复扔 n 次，如果问有 $x$ 次都是点数 6 朝上的概率就是：

$$
P = C(n,x)p^x(1-p)^{n-x}
$$

严格定义来说，将伯努利分布由单变量扩展到 $d$ 维向量 $x$，其中 $x_i \in {0,1}$，且$\sum_{i=1}^{d} x_i = 1$，并假设 $x_i$取 1 的概率为 $\mu_i \in [0,1]，\sum_{i=1}^d \mu_i = 1$，则将得到离散概率分布为:

$$
P( x | \mu) = \prod_{i=1}^d \mu_i^{x_i} \\
E[x_i] = \mu_i \\
Var[x_i] = \mu_i(1 - \mu_i) \\
Cov[x_j,x_i] = \prod[j=i] \mu_i
$$

在此基础上扩展二项分布则得到多项分布，它描述了在 $N$ 次独立实验中有 $m_i$ 次 $x_i = 1$ 的概率:

$$
P(m_1,m_2,...,m_d|N,\mu)=Mult(m_1,m_2,...,m_d | N,\mu) \\
= \frac{N!}{m_1!m_2!...m_d!}\prod_{i=1}^d \mu_i^{m_i} \\
E[m_i] = N_{\mu_i} \\
Var[m_i] = N_{\mu_i}(1-\mu_i) \\
Cov[m_j,m_i] = -N_{\mu_j \mu_i}
$$

# 正态分布/高斯分布

正态分布只依赖于数据集的两个特征：样本的均值和方差。正态分布的这种统计特性使得问题变得异常简单，任何具有正态分布的变量，都可以进行高精度分预测。值得注意的是，大自然中发现的变量，大多近似服从正态分布。即如果某个随机变量取值范围是实数，且对它的概率分布一无所知，通常会假设它服从正态分布，其考量的原因包括：

- 中心极限定理表明，多个独立随机变量的和近似正态分布，则建模的任务的真实分布通常都确实接近正态分布。

- 在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）。

## 一维正态分布

设 $X \sim N(\mu,\sigma^2)$，则其概率密度为：

$$
f(x)=\frac{1}{\sqrt{2\pi}*\sigma}e^{-\frac{(x-\mu)^2}{2*\sigma^2}}, -\infty<x<\infty
$$

其中 $\mu, \sigma(\sigma>0)$ 为常数，若随机变量 $X$ 的概率密度函数如上所述，则称 $X$ 服从参数为 $\mu, \sigma$ 的正态分布或者高斯分布，记作 $X \sim N\left(\mu, \sigma^{2}\right)$。

当 $\mu=0, \sigma=1$ 时，称为标准正态分布，其概率密度函数记作 $\varphi(x)$，分布函数记作 $\Phi(x)$。为了方便计算，有时记作 $\mathcal{N}\left(x ; \mu, \beta^{-1}\right)=\sqrt{\frac{\beta}{2 \pi}} \exp \left(-\frac{1}{2} \beta(x-\mu)^{2}\right)$，其中 $\beta \in(0, \infty)$。

正态分布的概率密度函数性质如下：

- 曲线关于 $x = \mu$ 处对称，并且在该处取到最大值。
- 曲线在 $x=\mu \pm \sigma$ 处有拐点，参数 $\mu$ 决定曲线的位置，$\sigma$ 决定图形的胖瘦。

# Beta 分布

Beta 分布的概率密度为：

$$
f(x) =
\left \{
\begin{aligned}
\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}, x \in (0,1) \\
0,其他
\end{aligned}
\right.
$$

其中$$B(\alpha,\beta) = \int_0^1 x^{\alpha - 1}(1-x)^{\beta-1}dx=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$$
其中 Gamma 函数可以看做阶乘的实数域的推广：

$$
\Gamma(x) = \int_0^{\infty}t^{x-1}e^{-t}dt \Rightarrow \Gamma(n) = (n-1)! \Rightarrow B(\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
$$

Beta 分布的期望为：
$$E(X) = \frac{\alpha + \beta}{\alpha}$$

# 泊松分布

设$X \sim \pi(\lambda)$，且分布律为：

$P\{X=k\}=\frac{\lambda^k}{k!}e^{-\lambda},k=0,1,2,…,\lambda>0$

则有

$$
E(X)=\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}e^{-\lambda} =\lambda
$$

$D(X)=\lambda$

在实际的事件中，当一个随机事件，以固定的平均瞬时速率$\lambda$或者所谓的密度随机且独立地出现时，那么这个事件在单位时间、面积或者体积内出现的次数或者个数就近似地服从泊松分布。

# 连续分布

# 指数分布
