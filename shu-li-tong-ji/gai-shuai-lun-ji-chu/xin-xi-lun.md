# 信息论

## 信息论

信息论背后的原理是从不太可能发生的事件中能学到更多的有用信息，发生可能性较大的事件包含较少的信息；发生可能性较小的事件包含较多的信息；独立事件包含额外的信息。

## 熵

信息熵反应了一个系统的有序化程度，一个系统越是有序，那么它的信息熵就越低，反之就越高。对于事件 $X=x$，定义自信息 Self-Information 为：$I\(x\)=-\log P\(x\)$。自信息仅仅处理单个输出，而熵就是为自信息的期望：

$$
H(X)=\mathbb{E}_{X \sim P(X)}[I(x)]=-\mathbb{E}_{X \sim P(X)}[\log P(x)]
$$

熵一般记作 $H\(P\)$。熵刻画了按照真实分布 来识别一个样本所需要的编码长度的期望（即平均编码长度），譬如含有 4 个字母 `(A,B,C,D)` 的样本集中，真实分布 $P=\left\(\frac{1}{2}, \frac{1}{2}, 0,0\right\)$，则只需要 1 位编码即可识别样本。对于离散型随机变量 $X$ ，假设其取值集合大小为 $K$ ，则可以证明：$0 \leq H\(X\) \leq \log K$ 。

### 条件熵

对于随机变量 $Y$ 和 $X$ ，条件熵 $H\(Y\|X\)$ 表示：已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。条件熵的定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的期望：

$$
H(Y | X)=\mathbb{E}_{X \sim P(X)}[H(Y | X=x)]=-\mathbb{E}_{(X, Y) \sim P(X, Y)} \log P(Y | X)
$$

对于离散型随机变量，存在：

$$
H(Y | X)=\sum_{x} p(x) H(Y | X=x)=-\sum_{x} \sum_{y} p(x, y) \log p(y | x)
$$

对于连续型随机变量，则存在：

$$
H(Y | X)=\int p(x) H(Y | X=x) d x=-\iint p(x, y) \log p(y | x) d x d y
$$

根据定义可以证明：

$$
H(X, Y)=H(Y | X)+H(X)
$$

即：描述 $X$ 和 $Y$ 所需要的信息是：描述 $X$ 所需要的信息加上给定 $X$ 条件下描述 $Y$ 所需的额外信息。

## KL 散度 & 相对熵

相对熵又称互熵，交叉熵，鉴别信息，Kullback 熵，Kullback-Leible 散度\(即 KL 散度\)等。设$p\(x\)$和$q\(x\)$是$x$取值的两个概率概率分布，则$p$对$q$的相对熵为

$$
D(p||q) = \sum\_{i=1}^{n}p(x_i)log\frac{p(x_i)}{q(x_i)}
$$

在一定程度上，熵可以度量两个随机变量的距离。KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度是用来度量使用基于 Q 的编码来编码来自 P 的样本平均所需的额外的位元数。 典型情况下，P 表示数据的真实分布，Q 表示数据的理论分布，模型分布，或 P 的近似分布。 相对熵\(KL 散度\)有两个主要的性质。如下 \(1\)尽管 KL 散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即$$D(p||q) \neq D(q||p)$$ \(2\)相对熵的值为非负值，即

$$
D(p||q) > 0
$$

**相对熵的应用**

相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵\(KL 散度\)可以用于比较文本的相似度，先统计出词的频率，然后计算 KL 散度就行了。另外，在多指标系统评估中，指标权重分配是一个重点和难点，通过相对熵可以处理。

#### 互信息

两个随机变量$X$，$Y$的互信息，定义为$X$，$Y$的联合分布和独立分布乘积的相对熵。 $$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$ $$I(X,Y)=\sum\_{x,y}log\frac{p(x,y)}{p(x)p(y)}$$

#### 信息增益

信息增益表示得知特征 A 的信息而使得类$X$的信息的不确定性减少的程度。信息增益的定义为特征$A$对训练数据集$D$的信息增益$g\(D,A\)$，定义为集合$D$的经验熵$H\(D\)$与特征$A$给定条件下$D$的经验条件熵$H\(D\|A\)$之差： $$g(D,A) = H(D) - H(D|A)$$

